#!/usr/bin/env python3
"""
Quick Performance Optimizations Script

Applies immediate optimizations for 30-40% latency reduction:
1. Use int8 quantization for Whisper
2. Reduce LLM max tokens
3. Apply torch.compile to CSM
4. Enable all native optimizations

Usage:
    python scripts/quick_optimizations.py
"""

import os
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

print("\n" + "=" * 80)
print("üöÄ FERN Quick Performance Optimizations")
print("=" * 80 + "\n")

print("This will apply the following optimizations:\n")
print("  1. ‚úÖ Whisper ASR: float16 ‚Üí int8 (2x faster)")
print("  2. ‚úÖ Gemini LLM: Unlimited ‚Üí 100 max tokens (30% faster)")  
print("  3. ‚úÖ CSM-1B TTS: Apply torch.compile (20-30% faster)")
print("  4. ‚úÖ Enable all native PyTorch optimizations")
print("\n" + "=" * 80)

proceed = input("\n‚ö†Ô∏è  This will modify your code. Proceed? (y/n): ")
if proceed.lower() != 'y':
    print("Aborted.")
    sys.exit(0)

print("\nüìù Applying optimizations...\n")

# ============================================================================
# 1. Optimize Whisper ASR (int8 quantization)
# ============================================================================
print("[1/4] Optimizing Whisper ASR...")

files_to_update = [
    "realtime_agent.py",
    "realtime_agent_advanced.py",
    "client_voice.py"
]

for filename in files_to_update:
    filepath = project_root / filename
    if not filepath.exists():
        print(f"  ‚äó Skipped {filename} (not found)")
        continue
    
    content = filepath.read_text()
    
    # Replace float16 with int8
    if 'compute_type = "float16"' in content:
        content = content.replace(
            'compute_type = "float16"',
            'compute_type = "int8"  # Optimized: 2x faster with minimal accuracy loss'
        )
        filepath.write_text(content)
        print(f"  ‚úì Updated {filename}: float16 ‚Üí int8")
    elif 'compute_type="float16"' in content:
        content = content.replace(
            'compute_type="float16"',
            'compute_type="int8"  # Optimized: 2x faster'
        )
        filepath.write_text(content)
        print(f"  ‚úì Updated {filename}: float16 ‚Üí int8")
    else:
        print(f"  ‚äó Skipped {filename} (already optimized or different format)")

# ============================================================================
# 2. Optimize Gemini LLM (reduce max tokens)
# ============================================================================
print("\n[2/4] Optimizing Gemini LLM...")

gemini_file = project_root / "fern" / "llm" / "gemini_manager.py"
if gemini_file.exists():
    content = gemini_file.read_text()
    
    # Add max_tokens parameter to __init__
    if "max_tokens: Optional[int] = None" not in content:
        # Find the __init__ method
        init_pattern = "def __init__(\n        self,"
        if init_pattern in content:
            content = content.replace(
                init_pattern,
                "def __init__(\n        self,\n        max_tokens: Optional[int] = 100,  # Optimized for low latency"
            )
            
            # Store max_tokens
            if "self.temperature = temperature" in content:
                content = content.replace(
                    "self.temperature = temperature",
                    "self.temperature = temperature\n        self.max_tokens = max_tokens  # Latency optimization"
                )
            
            # Use max_tokens in generation
            if "generation_config=" in content and "max_output_tokens" not in content:
                content = content.replace(
                    "generation_config=",
                    "generation_config={\n                'max_output_tokens': self.max_tokens,\n            },\n            generation_config="
                )
            
            gemini_file.write_text(content)
            print("  ‚úì Updated gemini_manager.py: Added max_tokens=100")
        else:
            print("  ‚äó Could not find __init__ method pattern")
    else:
        print("  ‚äó Already has max_tokens parameter")
else:
    print("  ‚äó Skipped (gemini_manager.py not found)")

# ============================================================================
# 3. Optimize CSM-1B TTS (torch.compile)
# ============================================================================
print("\n[3/4] Optimizing CSM-1B TTS...")

csm_file = project_root / "fern" / "tts" / "csm_real.py"
if csm_file.exists():
    content = csm_file.read_text()
    
    # Add torch.compile after loading model
    if "torch.compile" not in content and "self.generator," in content:
        # Find where generator is loaded
        load_pattern = "self.generator, self.mimi = load_csm_1b_real(device)"
        if load_pattern in content:
            content = content.replace(
                load_pattern,
                load_pattern + "\n            \n            # Apply torch.compile for 20-30% speedup\n            if device != 'cpu':  # torch.compile works best on GPU\n                logger.info('Applying torch.compile to CSM-1B...')\n                self.generator = torch.compile(self.generator, mode='reduce-overhead')"
            )
            csm_file.write_text(content)
            print("  ‚úì Updated csm_real.py: Added torch.compile")
        else:
            print("  ‚äó Could not find model loading pattern")
    else:
        print("  ‚äó Already uses torch.compile or different structure")
else:
    print("  ‚äó Skipped (csm_real.py not found)")

# ============================================================================
# 4. Apply native optimizations
# ============================================================================
print("\n[4/4] Applying native PyTorch optimizations...")

try:
    import torch
    
    # Enable TF32
    if torch.cuda.is_available():
        cuda_major = torch.cuda.get_device_capability()[0]
        if cuda_major >= 8:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print("  ‚úì TF32 enabled")
        else:
            print("  ‚äó TF32 not available (GPU too old)")
    
    # Enable cuDNN auto-tuner
    torch.backends.cudnn.benchmark = True
    print("  ‚úì cuDNN auto-tuner enabled")
    
    # Optimal threading
    num_threads = os.cpu_count()
    torch.set_num_threads(num_threads)
    print(f"  ‚úì PyTorch threads: {num_threads}")
    
    # Create persistent config
    config_file = project_root / "fern" / "config" / "optimizations.py"
    config_file.parent.mkdir(exist_ok=True)
    
    config_content = '''"""
Native PyTorch optimizations applied by quick_optimizations.py

Import this module at the start of your scripts for maximum performance.
"""

import torch
import os

# Enable TF32 for faster matmul on Ampere+ GPUs
if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

# Enable cuDNN auto-tuner
torch.backends.cudnn.benchmark = True

# Optimal threading
torch.set_num_threads(os.cpu_count())

# Memory allocator
if torch.cuda.is_available():
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

# JIT fusion
torch.jit.set_fusion_strategy([('STATIC', 20), ('DYNAMIC', 20)])

print("‚úì Native optimizations loaded")
'''
    
    config_file.write_text(config_content)
    print(f"  ‚úì Created {config_file.relative_to(project_root)}")
    
except Exception as e:
    print(f"  ‚ö†Ô∏è  Warning: {e}")

# ============================================================================
# Summary
# ============================================================================
print("\n" + "=" * 80)
print("‚úÖ Quick optimizations applied successfully!")
print("=" * 80)

print("\nüìä Expected Performance Improvements:\n")
print("  ‚Ä¢ Whisper ASR:  150ms ‚Üí 75ms  (50% faster)")
print("  ‚Ä¢ Gemini LLM:   350ms ‚Üí 245ms (30% faster)")
print("  ‚Ä¢ CSM-1B TTS:   400ms ‚Üí 300ms (25% faster)")
print("  ‚Ä¢ Total:        900ms ‚Üí 620ms (31% faster!)")

print("\nüéØ Next Steps:\n")
print("  1. Test the optimizations:")
print("     python realtime_agent_advanced.py")
print("\n  2. Fine-tune CSM-1B for voice cloning:")
print("     python scripts/train_lora.py --dataset Jinsaryko/Elise")
print("\n  3. Monitor performance:")
print("     # The advanced agent shows latency metrics")

print("\nüí° Further Optimizations:\n")
print("  ‚Ä¢ Use 'base' Whisper model:  75ms ‚Üí 25ms  (3x faster, slight accuracy loss)")
print("  ‚Ä¢ Local LLM (Llama 3.2 1B): 245ms ‚Üí 50ms  (5x faster)")
print("  ‚Ä¢ True streaming TTS:       300ms ‚Üí 150ms (2x faster perceived latency)")

print("\nüìö Documentation:\n")
print("  ‚Ä¢ Full audit: PERFORMANCE_AUDIT.md")
print("  ‚Ä¢ Troubleshooting: TROUBLESHOOTING.md")

print("\n" + "=" * 80 + "\n")

